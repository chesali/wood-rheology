{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ffdbb66-ddcf-4eea-aae3-46c10d3ca661",
   "metadata": {},
   "source": [
    "# Viscoelasticity\n",
    "<strong>J. M. Maas @ IfB, ETH Zürich, v. 2026-01-19</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efa6dd6-b343-4930-b67a-f2a6bcab368b",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    Select ▸▸ <strong>Restart the kernel and run all cells</strong> to visualize the creep curves and run a widget for the TMSP.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db485e6-e109-40e2-a8b4-03b6c041473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import ipywidgets as widgets\n",
    "%matplotlib widget\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Required modules\n",
    "import utils.rheonomic_utils as rhutils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786a7e6c-fbf7-4aeb-af1f-ab71b5b1b032",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data import\n",
    "---\n",
    "Import the fitted data curves, parameters, and the scleronomic tests' elastic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1a7f61-d51f-4216-b1e4-fbb11f232fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "input_folder = Path('data/04_viscoelasticity')\n",
    "figures_folder = Path('figures/04_viscoelasticity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236da20c-c820-4103-8d23-c05a6fe17934",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Collect files\n",
    "files_params = list( input_folder.glob('ve*_kv_fit_parameters.csv') )\n",
    "files_curves = list( input_folder.glob('ve*_kv_fit_strains.csv') )\n",
    "file_meta = input_folder / 'meta_information.ods'\n",
    "file_elasticity = input_folder / 'elasticity.xlsx'\n",
    "\n",
    "# Import creep strains\n",
    "rhs_curves = [float(file.stem.split('_')[0].strip('ve')) for file in files_curves]\n",
    "df_curves = pd.concat([pd.read_csv(file, index_col=['sample', 'component', 'times'], parse_dates=[2]) for file in files_curves], keys=rhs_curves, names=['rh'])\n",
    "df_curves.sort_index(inplace=True)\n",
    "\n",
    "# Import creep parameters\n",
    "rhs_params = [float(file.stem.split('_')[0].strip('ve')) for file in files_params]\n",
    "df_params = pd.concat([pd.read_csv(file, index_col=['sample', 'component']) for file in files_params], keys=rhs_params, names=['rh'])\n",
    "\n",
    "# Format creep dataframe\n",
    "str2list = lambda s: [float(a) for a in s.strip('[] ').split(' ') if a != '']\n",
    "df_params['tau'] = df_params['tau'].apply(str2list)\n",
    "df_params['g'] = df_params['g'].apply(str2list)\n",
    "df_params = df_params.explode(['tau', 'g']).set_index('tau', append=True).sort_index()\n",
    "\n",
    "# Import elasticity\n",
    "def import_elasticity(sheet):\n",
    "    df = pd.read_excel(file_elasticity, sheet_name=sheet, index_col=[0])\n",
    "    df = rhutils.interpolate_elasticity(df, 90)\n",
    "    return df\n",
    "    \n",
    "sheets_elasticity = ['compression', 'tension', 'shear', 'average']\n",
    "df_elasticity = pd.concat([import_elasticity(sheet) for sheet in sheets_elasticity], keys=sheets_elasticity)\n",
    "\n",
    "df_elasticity.loc['average', 'G_TR'] = df_elasticity.loc['average', 'G_RT'].to_numpy()\n",
    "df_elasticity.loc['average', 'G_LR'] = df_elasticity.loc['average', 'G_RL'].to_numpy()\n",
    "df_elasticity.loc['average', 'G_LT'] = df_elasticity.loc['average', 'G_TL'].to_numpy()\n",
    "\n",
    "# Show data\n",
    "display(df_curves)\n",
    "display(df_params)\n",
    "display(df_elasticity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabaa5d8-db0c-402d-a86e-d0dd2b10acb2",
   "metadata": {},
   "source": [
    "Import the moisture content of each experiment and match it to the correct samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64509fd1-9874-427f-a794-3c5f4e3ff4c7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Import moisture contents\n",
    "\n",
    "# Define import function\n",
    "def import_creep_moisture(file_meta, samples=None):\n",
    "    '''\n",
    "    Imports and processes moisture content data from a specified Excel file.\n",
    "\n",
    "    This function reads a spreadsheet containing moisture content data, cleans the data by removing \n",
    "    unnecessary columns and replacing invalid entries, calculates mean and standard deviation values \n",
    "    for each experimental condition, and fills missing data using group averages based on relative humidity (RH).\n",
    "    It also associates sample names with calculated moisture content values and computes summary statistics \n",
    "    (mean, standard deviation, minimum, and maximum) for moisture content grouped by RH.\n",
    "\n",
    "    Parameters:\n",
    "    file_meta (str): The file path to the Excel spreadsheet containing the data.\n",
    "    samples (list): Samples to include in the averaging. Average over all samples if None (default).\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - df_mc_samples (DataFrame): A DataFrame linking sample names to their moisture content means, \n",
    "          standard deviations, and RH values.\n",
    "        - mc_avg (DataFrame): A DataFrame containing summary statistics (mean, std, min, max) of \n",
    "          moisture content for each RH group.\n",
    "    '''\n",
    "    # Read spreadsheet\n",
    "    df_mc = pd.read_excel(file_meta, sheet_name='Masses_Twins', index_col=[0,1])\n",
    "    \n",
    "    # Cleanup table\n",
    "    df_mc.dropna(how='all', inplace=True)\n",
    "    df_mc.drop(columns=['Note'], inplace=True)\n",
    "    df_mc.replace('#VALUE!', np.nan, inplace=True)\n",
    "    df_mc.sort_index(inplace=True)\n",
    "    df_mc = df_mc.loc['Moisture content [-]']\n",
    "    \n",
    "    # Add additional information\n",
    "    df_mc['mean'], df_mc['std'] = df_mc.mean(axis=1), df_mc.std(axis=1)  # do not set subsequentially for noting including mean in the std calculation\n",
    "    df_mc['rh'] = [int(experiment.split('_')[1].strip('RH')) for experiment in df_mc.index]\n",
    "    \n",
    "    # Fill missing values\n",
    "    df_mc_mean = df_mc[['mean', 'rh']].groupby('rh').mean()\n",
    "    df_mc_std = df_mc[['mean', 'rh']].groupby('rh').std()\n",
    "    na_idx = df_mc['mean'].isna()\n",
    "    df_mc.loc[na_idx, 'mean'] = df_mc_mean.loc[df_mc.loc[na_idx, 'rh'], 'mean'].to_numpy()\n",
    "    df_mc.loc[na_idx, 'std'] = df_mc_std.loc[df_mc.loc[na_idx, 'rh'], 'mean'].to_numpy()\n",
    "    \n",
    "    # Import sample names\n",
    "    df_meta = rhutils.read_meta(file_meta, ['MACRO_' + name for name in df_mc.index.str.upper()])\n",
    "    \n",
    "    # Link sample names to moisture content\n",
    "    df_mc_samples = pd.DataFrame(data={'sample': df_meta['name'], 'experiment': df_meta.index.get_level_values('experiment').str.strip('MACRO_')})\n",
    "    df_mc_samples = pd.DataFrame(data={'sample': df_meta['name'].to_numpy(), 'experiment': df_meta.index.get_level_values('experiment').str.strip('MACRO_').str.replace('VISCO', 'Visco')})\n",
    "    df_mc_samples['mc_mean'] = df_mc.loc[df_mc_samples['experiment'], 'mean'].to_numpy()\n",
    "    df_mc_samples['mc_std'] = df_mc.loc[df_mc_samples['experiment'], 'std'].to_numpy()\n",
    "    df_mc_samples['rh'] = df_mc.loc[df_mc_samples['experiment'], 'rh'].to_numpy()\n",
    "    df_mc_samples.set_index('sample', inplace=True)\n",
    "    \n",
    "    # Calculate mean moisture content of valid samples\n",
    "    if samples is None:\n",
    "        samples = df_mc_samples.index.get_level_values('sample').unique()\n",
    "    mc_avg = df_mc_samples.loc[ samples, ['mc_mean', 'rh'] ].groupby('rh')['mc_mean'].agg(['mean', 'std', 'min', 'max']) * 100\n",
    "\n",
    "    return df_mc_samples, mc_avg\n",
    "\n",
    "\n",
    "# Call import function\n",
    "df_mc_samples, mc_avg = import_creep_moisture(\n",
    "    file_meta=file_meta,\n",
    "    samples=df_params.index.get_level_values('sample').unique()\n",
    ")\n",
    "\n",
    "# Show data\n",
    "display(df_mc_samples)\n",
    "display(mc_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710b5370-28df-4a30-b256-ea19115a0ea4",
   "metadata": {},
   "source": [
    "## Strain visualization\n",
    "---\n",
    "Visualize the strains and their corresponding Kelvin-Voigt series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2832df7-7835-442d-acc2-807b70741b4c",
   "metadata": {},
   "source": [
    "The Kelvin-Voigt series can be described as follows:\n",
    "\\begin{align}\n",
    "C_{ij}^{-1} (t) = C_{0,ij}^{-1} + C_{\\mathrm{ve},ij}^{-1} (t) = C_{0,ij}^{-1} \\left( 1 + \\sum_{k=1}^{N} g_k^\\mathrm{ve} \\left( 1 - \\mathrm{e}^{-t/\\tau_k} \\right) \\right) \\text{ with } C_{0,ij}^{-1} = \\frac{\\epsilon_{0,ij}}{\\sigma_{ij}} .\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7347f43e-6141-4f1c-aa90-2f8a8f9c885f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Reference curves\n",
    "refs_creep = dict()  # t in min\n",
    "\n",
    "# Curves from Hayashi et al. (1993)\n",
    "refs_creep['E_tR'] = { 'tmax': 8367, 'fun': lambda t: 4.60e-4 * (1 + (t/1.8e5)**0.273) }\n",
    "refs_creep['E_tL'] = { 'tmax': 5535, 'fun': lambda t: 5.92e-5 * (1 + (t/9.0e7)**0.242) }\n",
    "refs_creep['G_RL'] = { 'tmax': 6800, 'fun': lambda t: 7.43e-4 * (1 + (t/5.9e5)**0.272) }\n",
    "\n",
    "# Curves from Bengtsson et al. (2024)  # only tested over 24hrs and thus constant fit for larger times\n",
    "# refs_creep['G_RT'] = { 'tmax': 1440, 'fun': lambda t: 0.0199 + 0.01382*(1 - np.exp(-t/(1.333*24))) }\n",
    "# refs_creep['G_RL'] = { 'tmax': 1440, 'fun': lambda t: 1.434e-3 + 4.556e-5*(1 - np.exp(-t/(9.94*24))) }\n",
    "# refs_creep['G_TL'] = { 'tmax': 1440, 'fun': lambda t: 1.612e-3 + 8.6523e-5*(1 - np.exp(-t/(8.87*24))) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebdb2b6-1414-48d3-867e-e730125048a7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function definitions\n",
    "df_export_creep_strain = [pd.DataFrame()]\n",
    "df_export_creep_fit = [pd.DataFrame()]\n",
    "\n",
    "# Define strain plotting function\n",
    "def plot_strains(sample_type, component, ax, dfs=df_curves, dfp=df_params, dfe=df_elasticity, mclim=[0.07, 0.20],\n",
    "                 label=None, plot_exp_kw=dict(lw=0.6), plot_model_kw=dict(lw=1.2)):\n",
    "    \n",
    "    # Initialize formatting\n",
    "    times_max = 30  # in days\n",
    "    mcmin, mcmax = mclim\n",
    "    color = lambda mc: sns.color_palette('crest_r', as_cmap=True)(1-(mc - mcmin)/(mcmax - mcmin))\n",
    "    # color = lambda mc: 'black'\n",
    "    def get_ls(mc):\n",
    "        if mc < 0.1:\n",
    "            return ':'\n",
    "        elif mc < 0.15:\n",
    "            return '--'\n",
    "        else:\n",
    "            return '-'\n",
    "    \n",
    "    # Select elasticity table\n",
    "    def stype2index(sample_type):\n",
    "        if 'c' in sample_type:\n",
    "            return 'compression'\n",
    "        elif 't' in sample_type:\n",
    "            return 'tension'\n",
    "        elif 's' in sample_type:\n",
    "            return 'shear'\n",
    "        else:\n",
    "            raise ValueError(f'Invalid sample type {sample_type}.')\n",
    "\n",
    "    # Define moisture function\n",
    "    rh2moisture = lambda rhs: [rhutils.desorption_rh2mc(rh/100) if rh <= 65 else rhutils.adsorption_rh2mc(rh/100) for rh in rhs]\n",
    "\n",
    "    # Get relative compliance ratios\n",
    "    dfrc = get_elasticity_ratios(1/dfe, 65.0)  # reference relative humidity\n",
    "    \n",
    "    # Select data\n",
    "    dfs = dfs.loc[ [sample_type in s for s in dfs.index.get_level_values('sample')] ].query(f'component == \"{component}\"')\n",
    "    dfp = dfp.loc[ [sample_type in s for s in dfp.index.get_level_values('sample')] ].query(f'component == \"{component}\"')\n",
    "    dfe = dfe.loc[stype2index(sample_type)]\n",
    "    dfrc = dfrc.loc[stype2index(sample_type)]\n",
    "\n",
    "    # Iterate over data\n",
    "    samples = dfs.index.get_level_values('sample').unique()\n",
    "    for sample in samples:\n",
    "\n",
    "        # Get sample meta information\n",
    "        data_model = dfp.loc[(slice(None), sample, slice(None))]\n",
    "        eps0 = np.abs( data_model['eps0'].iloc[0] )\n",
    "        stress = np.abs( data_model['stress'].iloc[0] )\n",
    "        rh = data_model.index.get_level_values('rh')[0]\n",
    "        # mc = rh2moisture([rh])[0]\n",
    "        mc = df_mc_samples.loc[sample, 'mc_mean']\n",
    "        stiffness_scleronomic = np.abs( rhutils.get_stiffness(dfe, sample, component, rh) )\n",
    "        relative_compliance_scleronomic = np.abs( rhutils.get_stiffness(dfrc, sample, component, rh) )\n",
    "        normalize_compliance = lambda compliance: (compliance - 1)  # * eps0 / stress * stiffness_scleronomic # * relative_compliance_scleronomic\n",
    "        \n",
    "        # Plot experimental data\n",
    "        data_exp = dfs.loc[(slice(None), sample, slice(None))]\n",
    "        datetimes = data_exp.index.get_level_values('times')\n",
    "        times = (datetimes - datetimes[0]).total_seconds() / (3600*24)\n",
    "        compliance_exp = normalize_compliance(data_exp['strain_corrected'])\n",
    "        valid = ( np.abs(np.diff(compliance_exp, prepend=-1)) > 0 ) & ( times <= times_max )\n",
    "        ax.plot(times[valid], compliance_exp[valid], c=color(mc), **plot_exp_kw)\n",
    "\n",
    "        # Plot KV fitting data\n",
    "        compliance_model = normalize_compliance(data_exp['KV_model'])\n",
    "        ax.plot(times[valid], compliance_model[valid], c=color(mc), **plot_model_kw)\n",
    "\n",
    "    # Add legend\n",
    "    if not label is None:\n",
    "        label_color = 'cornflowerblue' if sample_type in ['cR', 'cT', 'cLx', 'sRT', 'sRL', 'sTL'] else 'chocolate'\n",
    "        text = ax.text(0.05, 0.955, label, transform=ax.transAxes, color=label_color, va='top', ha='left', fontsize='medium')\n",
    "        text.set_bbox(dict(facecolor='white', alpha=0.6, edgecolor='gray', boxstyle='round, pad=0.15, rounding_size=0.2'))\n",
    "\n",
    "    # Add R2\n",
    "    R2 = dfp['R2'].mean()\n",
    "    ax.text(0.98, 0.98, fr'$\\bar{{R}}^2 = {R2:.2f}$', transform=ax.transAxes, color='black', va='top', ha='right', fontsize='small')\n",
    "\n",
    "    # Format plot\n",
    "    ax.set_xlim([0, times_max])\n",
    "    ax.grid(True, which='major')\n",
    "    ax.grid(True, which='minor', ls='-')\n",
    "\n",
    "    df_export_creep_strain[0] = dfs\n",
    "    df_export_creep_fit[0] = dfp\n",
    "\n",
    "\n",
    "# Function for adding grid titles\n",
    "def set_grid_title(gs, pos, text, pad=6.0, **kwargs):\n",
    "    '''\n",
    "    Creates a title above a selected grid space of a gridspec.\n",
    "    Args:\n",
    "        gs       (mpl.gridspec.GridSpec) : Matplotlib gridspec where to take the grid quadrant from.\n",
    "        pos      (int, int)              : Vertical and horizontal quadrant index (y, x) as tuple.\n",
    "        text     (str)                   : Title label.\n",
    "        pad      (float)                 : Vertical padding between quadrant and text.\n",
    "        **kwargs (Text properties)       : Keywords passed to the text formatting.\n",
    "    '''\n",
    "    # Default values\n",
    "    fontsize = kwargs.get('fontsize', 12)\n",
    "    va = kwargs.get('va', 'bottom')\n",
    "    ha = kwargs.get('ha', 'center')\n",
    "    \n",
    "    # Collect gridspec information\n",
    "    y, x = pos\n",
    "    fig = gs.figure\n",
    "    geometry = gs.get_geometry()\n",
    "    gpos = gs.get_grid_positions(fig=fig)  # (bottom, top, left, right)\n",
    "\n",
    "    # Check for input error\n",
    "    if x >= geometry[1] or y >= geometry[0]:\n",
    "        raise ValueError(f'Invalid grid position ({(y,x)}) for grid of geometry {geometry}.')\n",
    "\n",
    "    # Transform coordinates\n",
    "    transform = fig.transFigure.inverted()\n",
    "    pad = transform.transform((0, pad))\n",
    "\n",
    "    # Select position\n",
    "    xcoord = (gpos[2][x] + gpos[3][x]) / 2\n",
    "    ycoord = gpos[1][y] + pad[1]\n",
    "\n",
    "    # Create text\n",
    "    fig.text(xcoord, ycoord, text, ha=ha, va=va, fontsize=fontsize, **kwargs)\n",
    "    return None\n",
    "\n",
    "\n",
    "# Function for getting relative elasticity ratios\n",
    "def get_elasticity_ratios(dfe, ref_rh):\n",
    "    '''\n",
    "    Return the ratios of elasticity in comparison to a specified reference relative humidity.\n",
    "    Input:\n",
    "        dfe     (pd.DataFrame) : Elasticity dataframe.\n",
    "        rel_rh  (float)        : Reference relative humidity; must be in index of dfe.\n",
    "    Output:\n",
    "        dfe_rel (pd.DataFrame) : Elasticity dataframe with relative ratios to reference relative humidity.\n",
    "    '''\n",
    "    # Initialize\n",
    "    sheets = dfe.index.get_level_values(0).unique()\n",
    "    columns = dfe.columns\n",
    "    dfe_rel = dfe.copy()\n",
    "\n",
    "    # Iterate over all sheets and columns\n",
    "    for sheet in sheets:\n",
    "        for column in columns:\n",
    "            # Calculate and store the relative values\n",
    "            ref_value = dfe.loc[(sheet, ref_rh), column]\n",
    "            dfe_rel.loc[sheet, column] = np.array( dfe.loc[sheet, column] / ref_value )\n",
    "\n",
    "    # Return result\n",
    "    return dfe_rel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a29133b-2b9b-4142-b69a-8c74a5614ffc",
   "metadata": {},
   "source": [
    "The next cell creates a widgets for exploring and exporting the individual strain data and their fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310c55dc-0ccf-4fe7-b00f-cc92a05c0e80",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Visualization function\n",
    "def visualize_creep(component, ax):\n",
    "    # Initialize\n",
    "    ax.cla()\n",
    "    component2label = {\n",
    "        'E_cRR': (r'$E_{c,R}^{-1}$'),\n",
    "        'E_tRR': (r'$E_{t,R}^{-1}$'),\n",
    "        'E_cTT': (r'$E_{c,T}^{-1}$'),\n",
    "        'E_tTT': (r'$E_{t,T}^{-1}$'),\n",
    "        'E_cLL': (r'$E_{c,L}^{-1}$'),\n",
    "        'E_tLL': (r'$E_{t,L}^{-1}$'),\n",
    "        'G_RT': (r'$G_{RT}^{-1}$'),\n",
    "        'G_TR': (r'$G_{TR}^{-1}$'),\n",
    "        'G_RL': (r'$G_{RL}^{-1}$'),\n",
    "        'G_LR': (r'$G_{LR}^{-1}$'),\n",
    "        'G_TL': (r'$G_{TL}^{-1}$'),\n",
    "        'G_LT': (r'$G_{LT}^{-1}$'),\n",
    "        'E_cRL': (r'$\\dfrac{ \\nu_{c, RL} }{ E_{c,R} }$'),\n",
    "        'E_tRL': (r'$\\dfrac{ \\nu_{t, RL} }{ E_{t,R} }$'),\n",
    "        'E_cTR': (r'$\\dfrac{ \\nu_{c, TR} }{ E_{c,T} }$'),\n",
    "        'E_tTR': (r'$\\dfrac{ \\nu_{c, TR} }{ E_{c,T} }$'),\n",
    "        'E_cLT': (r'$\\dfrac{ \\nu_{c, LT} }{ E_{c,L} }$'),\n",
    "        'E_tLT': (r'$\\dfrac{ \\nu_{t, LT} }{ E_{t,L} }$'),\n",
    "    }\n",
    "    \n",
    "    # Get input\n",
    "    constant = component.strip('KEG_')\n",
    "    cparts = component.split('_')\n",
    "    if 'E_' in component and (cparts[1][-2] != cparts[1][-1]):\n",
    "        direction = 'exx'\n",
    "        constant = constant[:-1]\n",
    "    elif 'G_' in component:\n",
    "        direction = 'exy'\n",
    "        constant = 's' + constant\n",
    "    else:\n",
    "        direction = 'eyy'\n",
    "        constant = constant[:-1]\n",
    "    if constant in ['cL', 'tL']:\n",
    "        constant += 'x'\n",
    "\n",
    "    # Create plot\n",
    "    plot_strains(constant, direction, ax, mclim=mclim, label=component2label[component], plot_exp_kw=plot_exp_kw, plot_model_kw=plot_model_kw)\n",
    "\n",
    "    # Format plot\n",
    "    ax.set_xlabel('Time [d]')\n",
    "    ax.set_ylabel(r'Creep compliance $C_{\\mathrm{ve},ij}^{-1} (t) / C_{0,ij}^{-1}$ [-]')\n",
    "    \n",
    "\n",
    "# Export function\n",
    "def export_csv_creep(button):\n",
    "    if len(df_export_creep_fit) > 0 and len(df_export_creep_strain) > 0:\n",
    "        export_path_fit = f'export_creep_fit_{w_creep_components.value}.csv'\n",
    "        export_path_strain = f'export_creep_points_{w_creep_components.value}.csv'\n",
    "        df_export_creep_fit[0].to_csv(export_path_fit)\n",
    "        df_export_creep_strain[0].to_csv(export_path_strain)\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        with w_creep.children[-1]:\n",
    "            w_creep.children[-1].clear_output()\n",
    "            print(f'({timestamp}) Successfully saved data to {export_path_fit} and {export_path_strain}.')\n",
    "button_creep = widgets.Button(description='Export CSVs', button_style='primary', icon='save')\n",
    "button_creep.on_click(export_csv_creep)\n",
    "display(button_creep)\n",
    "\n",
    "\n",
    "# Initialize formatting\n",
    "mclim = np.array([mc_avg['min'].min(), mc_avg['max'].max()]) / 100\n",
    "plot_exp_kw = dict(lw=0.6, ls=':', alpha=1.0)\n",
    "plot_model_kw = dict(lw=1.2, ls='-', alpha=0.8)\n",
    "\n",
    "# Create widgets\n",
    "fig, ax = plt.subplots()\n",
    "components = ['E_cRR', 'E_tRR', 'E_cTT', 'E_tTT', 'E_cLL', 'G_RT', 'G_TR', 'G_RL', 'G_LR', 'G_TL', 'G_LT', 'E_cRL', 'E_tRL', 'E_cTR', 'E_tTR', 'E_cLT', 'E_tLT']\n",
    "w_creep_components = widgets.Dropdown(options=components, description='component')\n",
    "w_creep = widgets.interactive(\n",
    "    visualize_creep,\n",
    "    component=w_creep_components,\n",
    "    ax=widgets.fixed(ax)\n",
    ")\n",
    "display(w_creep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9269629-6209-465c-8fd3-49de70fde1e2",
   "metadata": {},
   "source": [
    "Return the number of sample curves per anatomical direction and climate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fc5d53-08d9-4239-bf15-e100c9fc4fa7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Count number of curves per climate\n",
    "df_numbers = df_params.query(f'tau == {df_params.index.get_level_values(\"tau\").min()}').copy()\n",
    "df_numbers['direction'] = [sample.split('_')[1].split('-')[0] for sample in df_numbers.index.get_level_values('sample')]\n",
    "df_numbers = df_numbers.groupby(['rh', 'direction', 'component'])[['g']].count().sort_index()\n",
    "df_numbers.rename(columns={'g': 'number'}, inplace=True)\n",
    "df_numbers.query('direction != \"cL\" and direction != \"tL\"', inplace=True)\n",
    "\n",
    "df_numbers.to_csv(input_folder / 've_numbers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21e1795-2885-47b7-a839-2f571cf3897f",
   "metadata": {},
   "source": [
    "## Coefficient visualization\n",
    "---\n",
    "Visualize the distribution of Kelvin-Voigt coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f5b703-435f-43b9-9462-c7928d624c98",
   "metadata": {},
   "source": [
    "<strong style='color:red'>The Kelvin-Voigt coefficients in this plot got normalized to the scleronomic elastic compliance!</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a4fff7-afee-4550-9d8a-11e7b4fb0b4e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "\n",
    "# Initialize functions\n",
    "rh2moisture = lambda rhs: [rhutils.desorption_rh2mc(rh/100) if rh <= 65 else rhutils.adsorption_rh2mc(rh/100) for rh in rhs]\n",
    "mcmin, mcmax = 6, 21\n",
    "offset_fun = lambda mc: 0.55 * ((mc - mcmin) / (mcmax - mcmin) - 0.5)\n",
    "\n",
    "def stype2index(sample_type):\n",
    "    if 'c' in sample_type:\n",
    "        return 'compression'\n",
    "    elif 't' in sample_type:\n",
    "        return 'tension'\n",
    "    elif 's' in sample_type:\n",
    "        return 'shear'\n",
    "    else:\n",
    "        raise ValueError(f'Invalid sample type {sample_type}.')\n",
    "        \n",
    "def get_stiffness(df):\n",
    "    indexes = df.index\n",
    "    stiffnesses = []\n",
    "    for index in indexes:\n",
    "        stype = df.loc[index, 'type']\n",
    "        rh, sample, component, tau = index\n",
    "        stiffness = np.abs( rhutils.get_stiffness(df_elasticity.loc[stype2index(stype)], sample, component, rh) )\n",
    "        stiffnesses.append(stiffness)\n",
    "    return stiffnesses\n",
    "\n",
    "# Create DataFrame\n",
    "df = df_params.copy()\n",
    "df['type'] = [sample.split('_')[1].split('-')[0] + f'_{component}' for sample, component in zip(df.index.get_level_values('sample'), df.index.get_level_values('component'))]\n",
    "df['C_scleronomic'] = get_stiffness(df)\n",
    "df['tau'] = df_params.index.get_level_values('tau')\n",
    "df['g_abs'] = df_params['g'] # * np.abs( df_params['eps0'] / df_params['stress'] ) * df['C_scleronomic']\n",
    "# df.loc[(slice(None), slice(None), 'exx', slice(None)), 'g_abs'] = df.loc[(slice(None), slice(None), 'exx', slice(None)), 'g']\n",
    "# df.loc[(slice(None), slice(None), 'eyy', slice(None)), 'g_abs'] = df.loc[(slice(None), slice(None), 'eyy', slice(None)), 'g']\n",
    "df['mc'] = np.array( rh2moisture(df_params.index.get_level_values('rh')) ) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c0f7fa-5ba9-48cb-a058-81025755cc96",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Initialize plot\n",
    "margin = [0.06, 0.02, 0.10, 0.07]  # [left, right, bottom, top]\n",
    "fig, axs_all = plt.subplots(4, 2, figsize=(10, 6), width_ratios=[0.66, 0.34], gridspec_kw=dict(hspace=0, wspace=0, left=margin[0], right=1-margin[1], bottom=margin[2], top=1-margin[3]))\n",
    "axs, axsr = axs_all[:, 0], axs_all[:, 1]\n",
    "\n",
    "# Format left axes\n",
    "axs_top = np.array( [ax.twiny() for ax in axs] )\n",
    "[ax.tick_params(labelbottom=False) for ax in axs[:-1]]\n",
    "[ax.tick_params(labeltop=False) for ax in axs_top[1:]]\n",
    "\n",
    "# Format right axes\n",
    "axsr_top = np.array( [ax.twiny() for ax in axsr] )\n",
    "[ax.yaxis.set_label_position('right') for ax in axsr]\n",
    "[ax.yaxis.tick_right() for ax in axsr]\n",
    "[ax.tick_params(labelbottom=False) for ax in axsr[:-1]]\n",
    "[ax.tick_params(labeltop=False) for ax in axsr_top[1:]]\n",
    "\n",
    "\n",
    "# Plot settings\n",
    "fcs = ['lightsteelblue', 'sandybrown']\n",
    "ecs = ['cornflowerblue', 'chocolate']\n",
    "labels = [r'compression, $1/G_{RT}$, $1/G_{RL}$, $1/G_{TL}$',\n",
    "          r'tension, $1/G_{TR}$, $1/G_{LR}$, $1/G_{LT}$']\n",
    "alpha = [0.6, 0.5]\n",
    "box_width = 0.18\n",
    "ticks_fontsize = 'medium'\n",
    "\n",
    "\n",
    "# Iterate over taus\n",
    "taus = df['tau'].unique()\n",
    "for i, tau in enumerate(taus):\n",
    "    # Plot compression RTL data\n",
    "    types = ['cR_eyy', 'cT_eyy', 'cLx_eyy', 'sRT_exy', 'sRL_exy', 'sTL_exy']\n",
    "    data = df.query(f'type in {types} and tau == {tau}')\n",
    "    rhutils.boxplot(data, x='type', y='g_abs', hue='mc', order=types, legend=False, ax=axs[i], fc=fcs[0], ec=ecs[0], marker='o', box_width=box_width, lw=1.2, alpha=alpha[0], offset_fun=offset_fun)\n",
    "    \n",
    "    # Plot tension LTR data\n",
    "    types = ['tR_eyy', 'tT_eyy', 'tLx_eyy', 'sTR_exy', 'sLR_exy', 'sLT_exy']\n",
    "    data = df.query(f'type in {types} and tau == {tau}')\n",
    "    rhutils.boxplot(data, x='type', y='g_abs', hue='mc', order=types, legend=False, ax=axs_top[i], fc=fcs[1], ec=ecs[1], marker='o', box_width=box_width, lw=1.2, alpha=alpha[1], offset_fun=offset_fun)\n",
    "\n",
    "    # Plot lateral compression data\n",
    "    types = ['cT_exx', 'cR_exx', 'cLx_exx']\n",
    "    data = df.query(f'type in {types} and tau == {tau}')\n",
    "    rhutils.boxplot(data, x='type', y='g_abs', hue='mc', order=types, legend=False, ax=axsr[i], fc=fcs[0], ec=ecs[0], marker='o', box_width=box_width, lw=1.2, alpha=alpha[0], offset_fun=offset_fun)\n",
    "    \n",
    "    # Plot lateral tension data\n",
    "    types = ['tT_exx', 'tR_exx', 'tLx_exx']\n",
    "    data = df.query(f'type in {types} and tau == {tau}')\n",
    "    rhutils.boxplot(data, x='type', y='g_abs', hue='mc', order=types, legend=False, ax=axsr_top[i], fc=fcs[1], ec=ecs[1], marker='o', box_width=box_width, lw=1.2, alpha=alpha[1], offset_fun=offset_fun)\n",
    "\n",
    "\n",
    "# Add taus\n",
    "tau_xl, tau_xr, tau_y = 0.985, 0.97, 0.92\n",
    "tau_labels = [r'$\\tau_{1,ij} = 2\\,\\mathrm{h}$', r'$\\tau_{2,ij} = 20\\,\\mathrm{h}$', r'$\\tau_{3,ij} = 200\\,\\mathrm{h}$', r'$\\tau_{4,ij} = 2000\\,\\mathrm{h}$']\n",
    "tau_label_format = dict(va='top', ha='right', size='medium', weight='bold')\n",
    "bbox = dict(facecolor='white', alpha=0.6, edgecolor='gray', boxstyle='round, pad=0.15, rounding_size=0.2')\n",
    "for label, axl, axr in zip(tau_labels, axs, axsr):\n",
    "    textl = axl.text(tau_xl, tau_y, label, transform=axl.transAxes, **tau_label_format)\n",
    "    textr = axr.text(tau_xr, tau_y, label, transform=axr.transAxes, **tau_label_format)\n",
    "    textl.set_bbox(bbox)\n",
    "    textr.set_bbox(bbox)\n",
    "\n",
    "\n",
    "# Format left labels\n",
    "axs[0].text(0.02, 0.92, '(a)', transform=axs[0].transAxes, va='top', ha='left', weight='bold', size='x-large', family='arial')\n",
    "axs[-1].set_xticklabels([r'$E_{c,R}^{-1}$', r'$E_{c,T}^{-1}$', r'$E_{c,L}^{-1}$', r'$G_{RT}^{-1}$', r'$G_{RL}^{-1}$', r'$G_{TL}^{-1}$'], color=ecs[0], fontsize=ticks_fontsize)\n",
    "axs_top[0].set_xticklabels([r'$E_{t,R}^{-1}$', r'$E_{t,T}^{-1}$', r'$E_{t,L}^{-1}$', r'$G_{TR}^{-1}$', r'$G_{LR}^{-1}$', r'$G_{LT}^{-1}$'], color=ecs[1], fontsize=ticks_fontsize)\n",
    "def float2str(num):\n",
    "    exponent = np.ceil( np.log10( num ) ) - 1\n",
    "    base = num / 10**exponent\n",
    "    return f'{base:.1f} \\cdot 10^{int(exponent)}'\n",
    "# [ax.set_ylabel(rf'$\\tau_{i+1} = {int(tau/(3600))}$ h') for i, (ax, tau) in enumerate(zip(axs, taus))]\n",
    "# [ax.yaxis.set_label_position('right') for ax in axs]\n",
    "\n",
    "# Format right labels\n",
    "axsr[0].text(0.035, 0.92, '(b)', transform=axsr[0].transAxes, va='top', ha='left', weight='bold', size='x-large', family='arial')\n",
    "axsr[-1].set_xticklabels([r'$\\dfrac{\\nu_{c,TR}}{E_{c,T}}$', r'$\\dfrac{\\nu_{c,RL}}{E_{c,R}}$', r'$\\dfrac{\\nu_{c,LT}}{E_{c,L}}$'], color=ecs[0], fontsize=ticks_fontsize)\n",
    "axsr_top[0].set_xticklabels([r'$\\dfrac{\\nu_{t,TR}}{E_{t,T}}$', r'$\\dfrac{\\nu_{t,RL}}{E_{t,R}}$', r'$\\dfrac{\\nu_{t,LT}}{E_{t,L}}$'], color=ecs[1], fontsize=ticks_fontsize)\n",
    "\n",
    "# Format figure labels\n",
    "fig.supxlabel('Compliance component', x=(1-margin[1]+margin[0])/2, y=0.008)\n",
    "fig.supylabel(r'Kelvin-Voigt coefficients $g^\\mathrm{ve}_{k,ij}$ [-]', x=0.005, y=(1-margin[3]+margin[2])/2)\n",
    "\n",
    "# Format left grid\n",
    "ylims = [[0, 2.8], [0, 2.8], [0, 2.8], [0, 2.8]]\n",
    "steps = [0.2, 0.2, 0.2, 0.2]\n",
    "[ax.set_ylim(ylim) for ax, ylim in zip(axs, ylims)]\n",
    "axs[0].set_yticks(np.arange(0, 2.8, 1.0))\n",
    "axs[1].set_yticks(np.arange(0, 2.8, 1.0))\n",
    "axs[2].set_yticks(np.arange(0, 2.8, 1.0))\n",
    "axs[3].set_yticks(np.arange(0, 2.8, 1.0))\n",
    "[ax.set_yticks(np.arange(ylim[0]-ylim[0]%step, ylim[1]+1e-8, step), minor=True) for ax, ylim, step in zip(axs, ylims, steps)]\n",
    "[ax.grid(True, which='major') for ax in axs]\n",
    "[ax.grid(True, which='minor', ls=':', lw=0.5, color='lightgray') for ax in axs]\n",
    "\n",
    "# Format right grid\n",
    "ylimsr = [[0, 5.6], [0, 5.6], [0, 5.6], [0, 11.2]]\n",
    "stepsr = [0.4, 0.4, 0.4, 0.8]\n",
    "[ax.set_ylim(ylim) for ax, ylim in zip(axsr, ylimsr)]\n",
    "axsr[0].set_yticks(np.arange(0, 5.8, 2.0))\n",
    "axsr[1].set_yticks(np.arange(0, 5.8, 2.0))\n",
    "axsr[2].set_yticks(np.arange(0, 5.8, 2.0))\n",
    "axsr[3].set_yticks(np.arange(0, 12.0, 4.0))\n",
    "[ax.set_yticks(np.arange(ylim[0]-ylim[0]%step, ylim[1]+1e-8, step), minor=True) for ax, ylim, step in zip(axsr, ylimsr, stepsr)]\n",
    "[ax.grid(True, which='major') for ax in axsr]\n",
    "[ax.grid(True, which='minor', ls=':', lw=0.5, color='lightgray') for ax in axsr]\n",
    "\n",
    "\n",
    "# Add inset axis\n",
    "inset_base = [6, 1.0] # [x, y]\n",
    "inset_ax = axs_top[0]\n",
    "inset_color = 'black'\n",
    "inset_lw = 0.8\n",
    "inset_tickheight = 0.15\n",
    "inset_fontsize = 'medium'\n",
    "inset_ax.arrow(inset_base[0]+offset_fun(mcmin), inset_base[1], offset_fun(mcmax)-offset_fun(mcmin), 0, color=inset_color, lw=inset_lw, head_width=0.1, head_length=0.1, overhang=0.4)\n",
    "inset_ax.text(inset_base[0]+offset_fun(mcmax)+0.25, inset_base[1], r'$\\omega$', ha='center', va='center', fontsize=inset_fontsize)\n",
    "for i, mc in enumerate(df['mc'].unique()):\n",
    "    # Add tick lines\n",
    "    tick_x, tick_y = inset_base[0]+offset_fun(mc), inset_base[1]\n",
    "    inset_ax.plot([tick_x, tick_x], [tick_y-inset_tickheight/2, tick_y+inset_tickheight/2], color=inset_color, label='__no_legend__', lw=inset_lw)\n",
    "    # Add tick labels\n",
    "    if i % 2 == 0: # even ticks\n",
    "        inset_ax.text(tick_x, tick_y-inset_tickheight, f'{mc/100:.2f}', ha='center', va='top', color=inset_color, fontsize=inset_fontsize)\n",
    "    else: # odd ticks\n",
    "        inset_ax.text(tick_x, tick_y+inset_tickheight, f'{mc/100:.2f}', ha='center', va='bottom', color=inset_color, fontsize=inset_fontsize)\n",
    "\n",
    "\n",
    "# Save figure\n",
    "save_path = figures_folder / f'kv_coefficients.png'\n",
    "fig.savefig(save_path, dpi=300)\n",
    "print(f'Successfully saved figure under {save_path}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9e5704-2477-48e3-aff2-9e1cbbfd39e1",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "---\n",
    "To obtain the minimum number of required experiments, the fitted creep curves are subsequently clustered into groups of similar creep behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2ce729-c6e4-4315-9931-b98abfbd9065",
   "metadata": {},
   "source": [
    "The following clustering algorithm clusters the viscoelastic creep after 30 days per symmetrized direction, where the creep at the different RH are the respective features. For that purpose, the subsequent cell constructs the feature array, where the rows resemble the individual directions and the columns the features, i.e., the creep after 30 days for the individual RH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d91df6b-2e4e-4421-b1f1-defbd8bcc283",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Collect creep coefficients\n",
    "def get_avg_gi(constant, rh, symmetrize=False, df_p=df_params):\n",
    "    '''\n",
    "    Determine the average Kelvin-Voigt coefficients for a given constant.\n",
    "    Input:\n",
    "        constant   (str)          : Component of the creep compliance matrix.\n",
    "                                    Options: E_cRR, E_tRR, E_cTT, E_tTT, E_cLL, E_tLL,\n",
    "                                    G_RT, G_TR, G_RL, G_LR, G_TL, G_LT, E_cRL, E_tRL,\n",
    "                                    E_cTR, E_tTR, E_cLT, E_tLT\n",
    "        rh         (float)        : Relative humidity of group.\n",
    "        symmetrize (bool)         : Form average between selected constant and\n",
    "                                    complementary loading case.\n",
    "        df_p       (pd.DataFrame) : Dataframe with Kelvin-Voigt parameters per sample.\n",
    "    Output:\n",
    "        g_avg    (pd.Series)    : Average Kelvin-Voigt coefficients, indexed by tau.\n",
    "    Example:\n",
    "        get_avg_gi('G_RT', 35)\n",
    "    '''\n",
    "    # Clean data\n",
    "    df_p = df_p.loc[ [not ('_cL-' in sample or '_tL-' in sample) for sample in df_p.index.get_level_values('sample')] ]\n",
    "    \n",
    "    # Identify index\n",
    "    parts = constant.split('_')\n",
    "    match parts[0]:\n",
    "        case 'E':\n",
    "            loading_type = parts[1][0]\n",
    "            direction = parts[1][1:]\n",
    "            component = 'eyy' if direction[0] == direction[1] else 'exx'\n",
    "            direction = direction[0].upper()\n",
    "            sample_type = loading_type + direction\n",
    "            sample_type_complementary = ('t' if loading_type == 'c' else 'c') + direction\n",
    "        case 'G':\n",
    "            loading_type = 's'\n",
    "            direction = parts[1].upper()\n",
    "            component = 'exy'\n",
    "            sample_type = loading_type + direction\n",
    "            sample_type_complementary = loading_type + direction[::-1]\n",
    "        case _:\n",
    "            raise ValueError(f'Invalid constant \"{constant}\".')\n",
    "    \n",
    "    # Collect data\n",
    "    data = df_p.loc[rh].query(f'component == \"{component}\"')\n",
    "    samples = [sample_type in sample for sample in data.index.get_level_values('sample')]\n",
    "    samples_complementary = [sample_type_complementary in sample for sample in data.index.get_level_values('sample')]\n",
    "    data = data.loc[ (np.any([samples, samples_complementary], axis=0) if symmetrize else samples) ]\n",
    "\n",
    "    # Get average\n",
    "    data_mean = data.groupby('tau')[['g', 'C_inv', 'R2']].mean()\n",
    "    g_avg = data_mean['g']\n",
    "\n",
    "    # Return result\n",
    "    return g_avg\n",
    "\n",
    "\n",
    "# Collect cluster features\n",
    "def get_feature(constant, rhs, t=30):\n",
    "    '''\n",
    "    Collect cluster feature for given constant.\n",
    "    Input:\n",
    "        constant (str)        : Selected direction, which gets symmetrized with its\n",
    "                                complementary loading case.\n",
    "        rhs      (list)       : List of relative humidities, resembling the object features.\n",
    "        t        (float)      : Time in days where creep compliance is evaluated.\n",
    "    Output:\n",
    "        features (np.ndarray) : Creep compliance after t days at given rhs.\n",
    "    '''\n",
    "    # Initialize\n",
    "    t = np.array([t*24*60*60])  # transform time from days to seconds\n",
    "    \n",
    "    # Calculate creep after certain time for all climates\n",
    "    features = []\n",
    "    for rh in rhs:\n",
    "        G = get_avg_gi(constant, rh, symmetrize=True)\n",
    "        tau = G.index.to_numpy()\n",
    "        g = G.values\n",
    "        C_inv = np.sum( g*(1-np.exp(-np.array([t]).T/tau)), axis=1 )\n",
    "        features += list(C_inv)\n",
    "\n",
    "    # Return result\n",
    "    return features\n",
    "\n",
    "\n",
    "# Assemble features\n",
    "groups = ['E_cRR', 'E_cTT', 'E_cLL', 'G_RT', 'G_RL', 'G_TL', 'E_cTR']  # ignore 'E_cRL' and 'E_cLT' due to their inaccuracy\n",
    "rhs = sorted(df_params.index.get_level_values('rh').unique())\n",
    "cluster_time = 30  # time to determine clusters at in [days]\n",
    "features = [get_feature(group, rhs=rhs, t=cluster_time) for group in groups]  # columns = object features, rows = objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8fafe7-4add-489a-b96f-48d54255e5c4",
   "metadata": {},
   "source": [
    "Subsequently, form clusters from the selected features. Consider to omit $\\nu_{RL} / E_R$ and $\\nu_{LT} / E_L$ due to their low accuracy. The markers indicate the cluster values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6415b8e5-a84d-4e8a-8c21-cc8751d76233",
   "metadata": {},
   "source": [
    "The next cell creates a widget to explore different numbers of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb94196-2416-4667-ad0f-6f109de10d55",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create cluster exploration widget\n",
    "\n",
    "# Determine clusters\n",
    "def get_clusters(number_of_clusters, obs, whiten=False):\n",
    "    # Structure data to be clustered\n",
    "    if whiten:\n",
    "        obs_w = sp.cluster.vq.whiten(features) # normalizes data to improve the result of kmeans\n",
    "        obs_std = np.std(obs, axis=0)\n",
    "    else:\n",
    "        obs_w = obs\n",
    "        obs_std = 1.0\n",
    "    \n",
    "    # Perform cluster analysis\n",
    "    clust_pos, _ = sp.cluster.vq.kmeans(obs_w, number_of_clusters, iter=200, seed=0)\n",
    "    \n",
    "    # Read out the found clusters\n",
    "    clust_index, clust_distance = sp.cluster.vq.vq(obs_w, clust_pos)\n",
    "    clust_pos_original = clust_pos * obs_std # invert results back to non-normalized space\n",
    "\n",
    "    # Get maximum distance of objects of same cluster\n",
    "    dist_max = []\n",
    "    obs_dist = []\n",
    "    relative_distance = True\n",
    "    for i in sorted(np.unique(clust_index)):\n",
    "        objs = np.where(clust_index == i)[0]\n",
    "        coords = np.array(obs)[objs]\n",
    "        if relative_distance:\n",
    "            dists = np.array( [np.abs(coords[j] - coords[k]) / (np.abs(coords[j] + coords[k])/2) for j in range(len(coords)) for k in range(j, len(coords)) if j != k] )\n",
    "        else:\n",
    "            dists = np.array( [np.abs(coords[j] - coords[k]) for j in range(len(coords)) for k in range(j, len(coords)) if j != k] )\n",
    "        obs_dist.append(dists if len(dists) > 0 else np.zeros((1, np.shape(obs_w)[1])))\n",
    "        \n",
    "    obs_dist_mean = np.array( [np.mean(a) for a in obs_dist] )\n",
    "    obs_dist_max = np.array( [np.max(a, axis=0) for a in obs_dist] )\n",
    "\n",
    "    # Return result\n",
    "    clusters = {\n",
    "        'clust_index': clust_index,\n",
    "        'clust_pos': clust_pos,\n",
    "        'clust_pos_original': clust_pos_original,\n",
    "        'clust_distance': clust_distance,\n",
    "        'obs_mean_distance': obs_dist_mean,  # mean distance between the objects of the respective group\n",
    "        'obs_max_distance': obs_dist_max  # maximum distance between the objects of the respective group\n",
    "    }\n",
    "    return clusters\n",
    "\n",
    "\n",
    "# Visualization function\n",
    "def visualize_clusters(number_of_clusters, axs):\n",
    "    # Initialize\n",
    "    [ax.cla() for ax in axs.flatten()]\n",
    "    \n",
    "    # Read out results\n",
    "    cluster = get_clusters(number_of_clusters, features)\n",
    "    clust_index, clust_pos_original, clust_distance = cluster['clust_index'], cluster['clust_pos_original'], cluster['clust_distance']\n",
    "    obj_max_distance = cluster['obs_max_distance']\n",
    "    \n",
    "    around = lambda a: np.round(a, 3)\n",
    "    clust_out = 'Cluster memberships:\\n' + '\\n'.join(\n",
    "        [f' {group}: {clust_index[i]:d} (cluster value: {around(clust_pos_original[clust_index[i]])}, ' + \\\n",
    "         f'maximum distance between objects: {around(obj_max_distance[clust_index[i]])}, ' + \\\n",
    "         f'cluster distance: {clust_distance[i]:.3f})' for i, group in enumerate(groups)\n",
    "        ])\n",
    "    print(clust_out)\n",
    "    print(f'Mean cluster distance: {np.mean(clust_distance):.3f}.')\n",
    "    print(f'Maximum cluster distance: {np.max(clust_distance):.3f}.')\n",
    "    \n",
    "    # Plot cluster\n",
    "    group_formats = {\n",
    "        'E_cRR': dict(pos=[0, 0]),\n",
    "        'E_cTT': dict(pos=[1, 0]),\n",
    "        'E_cLL': dict(pos=[2, 0]),\n",
    "        'G_RT': dict(pos=[0, 1]),\n",
    "        'G_RL': dict(pos=[1, 1]),\n",
    "        'G_TL': dict(pos=[2, 1]),\n",
    "        'E_cTR': dict(pos=[0, 2]),\n",
    "        'E_cRL': dict(pos=[1, 2]),\n",
    "        'E_cLT': dict(pos=[2, 2]),\n",
    "    }\n",
    "    cmap = mpl.colormaps['tab10']\n",
    "    rhs = sorted(df_params.index.get_level_values('rh').unique())\n",
    "    \n",
    "    for i, group in enumerate(groups):\n",
    "        # Select formatting\n",
    "        pos = group_formats[group]['pos']\n",
    "        ax = axs[pos[0], pos[1]]\n",
    "        cluster = clust_index[i]\n",
    "        color = cmap(cluster)\n",
    "        time_factor = 24*60*60  # transform days to seconds\n",
    "    \n",
    "        # Determine creep curves\n",
    "        for j, rh in enumerate(rhs):\n",
    "            G_i = get_avg_gi(group, rh, symmetrize=True)\n",
    "            g = G_i.values\n",
    "            tau = G_i.index.to_numpy()\n",
    "            times = np.linspace(0, cluster_time*time_factor, 100)\n",
    "            compliances = np.sum( g*(1-np.exp(-np.array([times]).T/tau)), axis=1 )\n",
    "        \n",
    "            # Plot curve\n",
    "            label = rf'${group.replace(\"c\", \"\").replace(\"t\", \"\").replace(\"_\", \"_{\")}}}$ (cluster {cluster})' if j == 0 else '__no_label__'\n",
    "            ax.plot(times/time_factor, compliances, color=color, label=label)\n",
    "    \n",
    "        # Plot cluster result\n",
    "        clust_result = clust_pos_original[cluster]\n",
    "        ax.scatter(np.full(clust_result.shape, cluster_time), clust_result, s=30, marker='o', fc=(0, 0, 0, 0), color=color, zorder=10)\n",
    "    \n",
    "        # Format plot\n",
    "        ax.legend(loc='upper left')\n",
    "        ax.grid(True)\n",
    "\n",
    "\n",
    "# Initialize figure\n",
    "fig, axs = plt.subplots(3, 3, figsize=(9, 7), gridspec_kw=dict(hspace=0, wspace=0), sharex=True, sharey=True)\n",
    "fig.suptitle('Clustering')\n",
    "fig.supxlabel('Time [d]')\n",
    "fig.supylabel('Normalize creep compliance [-]')\n",
    "fig.tight_layout()\n",
    "\n",
    "# Create widget\n",
    "widgets.interactive(\n",
    "    visualize_clusters,\n",
    "    number_of_clusters=widgets.Dropdown(value=4, options=np.arange(0, 7, dtype=int)),\n",
    "    axs=widgets.fixed(axs)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db22a2c-c154-420c-89b6-a9f9c0159e7e",
   "metadata": {},
   "source": [
    "The next cell determines the increment of cluster distance over increasing number of clusters. For a minimum number of experiments, it would be advisable to keep the number of clusters as low as necessary. <i>err_mean</i> and <i>err_max</i> denote the mean and maximum relative difference between the creep compliances of a same group. <!--, and <i>i_err_max</i> denotes the cluster number where this maximum difference occurs.-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b47e2d5-87dd-44dc-871e-2b29c6bcfa1a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Iterate over different numbers of clusters\n",
    "numbers_of_clusters = list( range(1, len(groups)+1) )\n",
    "\n",
    "clusters = [get_clusters(n, features) for n in numbers_of_clusters]\n",
    "indexes = [cluster['clust_index'] for cluster in clusters]\n",
    "distances = [cluster['clust_distance'] for cluster in clusters]\n",
    "obj_mean_distances = [cluster['obs_mean_distance'] for cluster in clusters]\n",
    "obj_max_distances = [cluster['obs_max_distance'] for cluster in clusters]\n",
    "\n",
    "d_mean = [np.mean(d) for d in distances]\n",
    "d_max = [np.max(d) for d in distances]\n",
    "err_mean = [np.round( d, 3) for d in obj_mean_distances]\n",
    "err_max = [np.round( np.max(d, axis=1), 3) for d in obj_max_distances]\n",
    "err_mean_global = [np.mean(d) for d in obj_max_distances]\n",
    "err_max_global = [np.max(d) for d in obj_max_distances]\n",
    "# i_err_max = [np.unravel_index(np.argmax(d), d.shape)[0] for d in obj_max_distances]\n",
    "\n",
    "# Assemble results\n",
    "labels = [group.replace('c', '').replace('t', '') for group in groups]\n",
    "dict_index = dict(zip(['i_'+label for label in labels], np.array(indexes).T))\n",
    "dict_dist = dict(zip(['dist_'+label for label in labels], np.array(distances).T))\n",
    "clust_df = pd.DataFrame(\n",
    "    data=dict_index | dict_dist,\n",
    "    index=pd.Index(numbers_of_clusters, name='cluster')\n",
    ")\n",
    "clust_df['dist_mean'] = d_mean\n",
    "clust_df['dist_max'] = d_max\n",
    "clust_df['delta_dist_mean'] = clust_df['dist_mean'].diff()\n",
    "clust_df['delta_dist_max'] = clust_df['dist_max'].diff()\n",
    "clust_df['err_mean'] = err_mean\n",
    "clust_df['err_max'] = err_max\n",
    "clust_df['err_mean_global'] = err_mean_global\n",
    "clust_df['err_max_global'] = err_max_global\n",
    "# clust_df['i_err_max'] = i_err_max\n",
    "\n",
    "# Return results\n",
    "display(clust_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8472a6c-472f-4c20-b808-ce360cdd0f6a",
   "metadata": {},
   "source": [
    "## Time-moisture superposition\n",
    "---\n",
    "Visualize the results of the time-moisture superposition principle (TMSP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380d6d50-529e-434b-accd-c2d6f41bd0a7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Import data tables\n",
    "tmsp_folder = input_folder / 'tmsp'\n",
    "\n",
    "# Parameters\n",
    "df_master = pd.read_csv(tmsp_folder / 'tmsp_master_curve_parameters.csv', index_col=['constant', 'element'])\n",
    "df_time_shift = pd.read_csv(tmsp_folder / 'tmsp_time_shift_parameters.csv', index_col=['constant'])\n",
    "df_ln_a = pd.read_csv(tmsp_folder / 'tmsp_time_shifts.csv', index_col=['constant', 'sample'])\n",
    "\n",
    "# Sample information\n",
    "df_meta = pd.read_csv(tmsp_folder / 'tmsp_meta.csv', index_col=['sample', 'component'])\n",
    "df_creep = pd.read_csv(tmsp_folder / 'tmsp_sample_strains.csv', index_col=['sample', 'component', 'times'], parse_dates=['times'])\n",
    "\n",
    "# Model response\n",
    "df_models_master = pd.read_csv(tmsp_folder / 'tmsp_master_models.csv', index_col=['component', 'x'])\n",
    "df_models_time_shift = pd.read_csv(tmsp_folder / 'tmsp_time_shift_models.csv', index_col=['component', 'x'])\n",
    "df_log = pd.read_csv(tmsp_folder / 'tmsp_log.csv', index_col=['constant', 'iteration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531fac1a-7c8a-4b4c-855a-90e6923c1c8b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define visualization function\n",
    "df_export_tmsp_master = [pd.DataFrame()]\n",
    "df_export_tmsp_ln_a = [pd.DataFrame()]\n",
    "\n",
    "# Initialize settings\n",
    "kv_model = lambda t, gamma_0, g, tau: gamma_0 * ( 1 + np.sum( g*(1-np.exp(-np.array([t]).T/tau)), axis=1 ) )\n",
    "settings = dict()\n",
    "settings['mode'] = 'experimental'  # data source\n",
    "settings['mc_ref'] = 12.5  # reference moisture content\n",
    "settings['fun_ln_a'] = lambda x, p, exp_mc, exp_ln_a: p[1] + (np.max(exp_ln_a[np.where(exp_mc > 17)[0]]) - p[1]) / (1+np.exp(-1*(x-p[0])))  # logistic function\n",
    "settings['fun_ln_a_extra_params'] = lambda exp_mc, exp_ln_a: {  # extra parameters to be stored in dataframe during fitting\n",
    "    'y_r': np.max(exp_ln_a[np.where(exp_mc > 17)[0]]),\n",
    "}\n",
    "settings['fun_ln_a_p0'] = lambda exp_mc, exp_ln_a: [12.5, np.mean(exp_ln_a[np.where(exp_mc < 10)[0]])]\n",
    "settings['fun_ln_a_bounds'] = lambda exp_mc, exp_ln_a: [ (10, 20), (np.min(exp_ln_a), np.max(exp_ln_a)) ]\n",
    "settings['compliance_type'] = 'J_relative'\n",
    "\n",
    "match settings['compliance_type']:  # adapt function kv_model(t, gamma_0, g, tau) to selected compliance type\n",
    "    case 'J_total':\n",
    "        settings['fun_master_curve'] = lambda t, tau, p: kv_model( t, p[0], p[1:], tau )\n",
    "    case 'J_c':\n",
    "        settings['fun_master_curve'] = lambda t, tau, p: kv_model( t, 1, p, tau ) - 1\n",
    "    case 'J_relative':\n",
    "        settings['fun_master_curve'] = lambda t, tau, p: kv_model( t, 1, p, tau ) - 1\n",
    "    case _:\n",
    "        raise ValueError(f\"Invalid compliance type '{settings['compliance_type']}'.\")\n",
    "\n",
    "\n",
    "# Transform constant to samples\n",
    "def extract_samples(df_meta, constant):\n",
    "    # Collect matching sasmples\n",
    "    data = df_meta.query(f\"constant == '{constant}'\")\n",
    "    samples = sorted( data.index.get_level_values('sample').unique() )\n",
    "    components = sorted( data.index.get_level_values('component').unique() )\n",
    "\n",
    "    # Check found data\n",
    "    if len(components) != 1:\n",
    "        raise ValueError(f'Invalid number of components found. Expected one, but found {components}')\n",
    "    component = components[0]\n",
    "\n",
    "    # Return results\n",
    "    return samples, component\n",
    "\n",
    "\n",
    "# Collect strains\n",
    "def collect_strains(sample, component, mode, df_creep):\n",
    "    '''\n",
    "    Collects the creep strains for a given samples and component.\n",
    "    Sections with zero strain changes are removed from data.\n",
    "    Input:\n",
    "        sample    (str)        : Sample name to collect the strains for.\n",
    "        component (str)        : Strain component, i.e., 'exx', 'exy', or 'eyy'.\n",
    "        mode      (str)        : Collection mode, i.e., 'experimental', 'fitted', or 'average'.\n",
    "    Output:\n",
    "        times     (np.ndarray) : Times in seconds.\n",
    "        strains   (np.ndarray) : Absolute creep strains.\n",
    "    '''\n",
    "    # Select time series with strain data\n",
    "    df = df_creep.loc[(sample, component)]\n",
    "    compliance = df_meta.loc[(sample, component), 'compliance']\n",
    "    match mode:\n",
    "        case 'experimental':\n",
    "            times = df.index.get_level_values('times')\n",
    "            times = (times - times[0]).total_seconds()\n",
    "            strains = df['strain_experiment'].to_numpy() * compliance\n",
    "        case 'fitted':\n",
    "            times = df.index.get_level_values('times')\n",
    "            times = (times - times[0]).total_seconds()\n",
    "            strains = df['KV_model'].to_numpy() * compliance\n",
    "        case 'average':\n",
    "            # Calculate from average fitted parameters\n",
    "            constant = df_meta.loc[(sample, component), 'constant']\n",
    "            rh = df_meta.loc[(sample, component), 'rh']\n",
    "            df = df_creep_avg.loc[(constant, rh)]\n",
    "            compliance = 1 / df_meta.query(f'constant == \"{constant}\" and rh == {rh}')['stiffness'].mean()\n",
    "            times = df.index.get_level_values('times')\n",
    "            strains = df['strain'].to_numpy() * compliance\n",
    "        case _:\n",
    "            raise ValueError(f\"Invalid mode '{mode}'.\")\n",
    "    # Crop constant strains\n",
    "    non_zero_diff = np.where( ~np.isclose(np.diff(strains, prepend=0), 0) )[0]\n",
    "    times = times[non_zero_diff]\n",
    "    strains = strains[non_zero_diff]\n",
    "    # Adapt to compliance type:\n",
    "    match settings['compliance_type']:\n",
    "        case 'J_total':\n",
    "            pass\n",
    "        case 'J_relative':\n",
    "            strains = strains / strains[0] - 1\n",
    "        case 'J_c':\n",
    "            strains = strains - strains[0]  # not necessary according to Hajikarimi et al. (2018)\n",
    "        case _:\n",
    "            raise ValueError(f\"Invalid compliance type '{settings['compliance_type']}'.\")\n",
    "    # Return result\n",
    "    return times, strains\n",
    "\n",
    "\n",
    "# Define visualization function\n",
    "def visualize_tmsp_model(constant, moisture, show_report, show_confidence, df_master, df_time_shift, df_ln_a, df_log, df_models_master, df_models_time_shift, df_meta, df_creep, axs):\n",
    "    # Initialize\n",
    "    mode = settings['mode']\n",
    "    model_color = 'red'\n",
    "    confidence_color = 'gray'\n",
    "    model_lw = 2.5\n",
    "    sigma = 1  # standard deviation multiplier for confidence bands and prediction bands\n",
    "    [ax.cla() for ax in axs]\n",
    "    mcmin, mcmax = df_meta['mc'].min(), df_meta['mc'].max()\n",
    "    mc2color = lambda mc: sns.color_palette('crest_r', as_cmap=True)(1-(mc - mcmin)/(mcmax - mcmin))\n",
    "\n",
    "    model_master = df_models_master.loc[constant]\n",
    "    model_time_shift = df_models_time_shift.loc[constant]\n",
    "\n",
    "    # Show report\n",
    "    if show_report:\n",
    "        display(df_log.loc[constant])\n",
    "    else:\n",
    "        print('')\n",
    "\n",
    "    # Collect samples\n",
    "    samples, component = extract_samples(df_meta, constant)\n",
    "    mcc_mean = df_meta.query(f'component == \"{component}\"').groupby(['constant', 'rh'])['mc'].agg(['mean', 'std'])\n",
    "\n",
    "    # Iterate over all samples\n",
    "    time_exp, time_mapped_exp = [], []\n",
    "    exp_data_master = []\n",
    "    for sample in samples:\n",
    "        # Initialize values\n",
    "        ln_a = df_ln_a.loc[(constant, sample), 'ln_a']\n",
    "        mc = df_ln_a.loc[(constant, sample), 'mc']\n",
    "        rh = df_meta.loc[(sample, component), 'rh']\n",
    "        mc_mean = mcc_mean.loc[(constant, rh), 'mean']\n",
    "        color = mc2color(mc)\n",
    "        \n",
    "        # Collect experimental creep\n",
    "        time, strain = collect_strains(sample, component, mode, df_creep)\n",
    "        valid_idx = np.where( ~np.isclose(time, 0) )\n",
    "        time = time[valid_idx]\n",
    "        strain = strain[valid_idx]\n",
    "\n",
    "        # Plot experimental creep\n",
    "        axs[0].plot(time, strain, label=f'exp. ({mc_mean:.1f}%)', color=color, lw=1, ls='--')\n",
    "\n",
    "        # Plot experimental creep in master time domain\n",
    "        time_mapped = np.exp(ln_a) * time\n",
    "        axs[1].scatter(np.log(time_mapped), strain, color=color, s=5, zorder=4)\n",
    "\n",
    "        # Plot experimental time shifts\n",
    "        axs[2].scatter([mc], [ln_a], color=color, zorder=4)\n",
    "\n",
    "        # Store information\n",
    "        time_exp.append(time)\n",
    "        time_mapped_exp.append(time_mapped)\n",
    "        exp_data_master.append(\n",
    "            pd.DataFrame(\n",
    "                data={'strain': strain},\n",
    "                index=pd.MultiIndex.from_tuples([(sample, t) for t in np.log(time_mapped)], names=['sample', 'log(time)'])\n",
    "            )\n",
    "        )\n",
    "\n",
    "    time_exp = np.concatenate(time_exp)\n",
    "    time_mapped_exp = np.concatenate(time_mapped_exp)\n",
    "    exp_data_master = pd.concat(exp_data_master)\n",
    "\n",
    "    \n",
    "    # Calculate master creep curve\n",
    "    g = df_master.loc[constant, 'g'].to_numpy()\n",
    "    tau = df_master.loc[constant, 'tau'].to_numpy()\n",
    "    if settings['compliance_type'] == 'J_total':\n",
    "        tau = tau[1:]\n",
    "    time_mcc = np.geomspace(1, np.max(time_mapped_exp), 100)\n",
    "    fun_mcc = lambda time: settings['fun_master_curve'](time, tau, g)\n",
    "    strain_mcc = fun_mcc(time_mcc)\n",
    "    time_mcc_model = model_master.index\n",
    "    strain_mcc_model = model_master['fit']\n",
    "    confidence_mcc = model_master['confidence']\n",
    "    prediction_mcc = model_master['prediction']\n",
    "    # confidence_mcc = model_master.eval_uncertainty(x=time_mcc, sigma=sigma)\n",
    "    # prediction_mcc = model_master.dely_predicted\n",
    "\n",
    "    # Plot master creep curve\n",
    "    axs[1].plot(np.log(time_mcc), strain_mcc, color='k', lw=model_lw, zorder=5)\n",
    "    if show_confidence:\n",
    "        axs[1].fill_between(np.log(time_mcc_model), strain_mcc_model-confidence_mcc, strain_mcc_model+confidence_mcc, zorder=2, color=confidence_color, ec='none', alpha=0.5)\n",
    "        axs[1].fill_between(np.log(time_mcc_model), strain_mcc_model-prediction_mcc, strain_mcc_model+prediction_mcc, zorder=1, color=confidence_color, ec='none', alpha=0.2)\n",
    "\n",
    "    # Calculate time-shift curve\n",
    "    p = df_time_shift.loc[constant, [c for c in df_time_shift.columns.str.lower() if 'p_' in c and not 'std' in c]].to_numpy()\n",
    "    mc_ts = np.linspace(0, 25, 100)\n",
    "    fun_ts = lambda mc: settings['fun_ln_a'](mc, p, df_ln_a.loc[constant, 'mc'].to_numpy(), df_ln_a.loc[constant, 'ln_a'].to_numpy())\n",
    "    ln_a_ts = fun_ts(mc_ts)\n",
    "    mc_ts_model = model_time_shift.index\n",
    "    ln_a_ts_model = model_time_shift['fit']\n",
    "    confidence_ts = model_time_shift['confidence']\n",
    "    prediction_ts = model_time_shift['prediction']\n",
    "    # confidence_ts = model_time_shift.eval_uncertainty(x=mc_ts, sigma=sigma)\n",
    "    # prediction_ts = model_time_shift.dely_predicted\n",
    "\n",
    "    # Plot time-shift function\n",
    "    axs[2].plot(mc_ts, ln_a_ts, color='k', zorder=5)\n",
    "    if show_confidence:\n",
    "        axs[2].fill_between(mc_ts_model, ln_a_ts_model-confidence_ts, ln_a_ts_model+confidence_ts, zorder=2, color=confidence_color, ec='none', alpha=0.5)\n",
    "        axs[2].fill_between(mc_ts_model, ln_a_ts_model-prediction_ts, ln_a_ts_model+prediction_ts, zorder=1, color=confidence_color, ec='none', alpha=0.2)\n",
    "    \n",
    "\n",
    "    # Plot selected moisture content\n",
    "    ln_a = fun_ts(moisture)\n",
    "    axs[2].scatter([moisture], [ln_a], marker='s', color=model_color, s=50, zorder=10)\n",
    "\n",
    "    # Plot creep model at selected moisture content\n",
    "    time_exp = np.geomspace(0.1, np.max(time_exp), 100)\n",
    "    time_exp_mcc = np.exp(ln_a) * time_exp\n",
    "    strain_exp = fun_mcc(time_exp_mcc)\n",
    "    axs[0].plot(time_exp, strain_exp, label=f'model ({moisture:.1f}%)', color=model_color, lw=model_lw, zorder=10)\n",
    "\n",
    "    # Show section of selected moisture content in master creep curve\n",
    "    axs[1].plot(np.log(time_exp_mcc), strain_exp, color=model_color, lw=model_lw, zorder=10)\n",
    "    \n",
    "\n",
    "    # Plot creep model at experiment climates\n",
    "    mc_exp_mean = df_meta.query(f'constant == \"{constant}\"').loc[samples].groupby('rh')['mc'].mean()\n",
    "    for mc in sorted(mc_exp_mean):\n",
    "        ln_a = fun_ts(mc)\n",
    "        strain_exp = fun_mcc(np.exp(ln_a) * time_exp)\n",
    "        axs[0].plot(time_exp, strain_exp, label=f'model ({mc:.1f}%)', color=mc2color(mc), lw=model_lw)\n",
    "\n",
    "\n",
    "    # Format axes\n",
    "    axs[1].sharey(axs[0])\n",
    "    axs[1].tick_params(labelleft=False)\n",
    "    [ax.grid(True) for ax in axs]\n",
    "\n",
    "    # Select label\n",
    "    match settings['compliance_type']:\n",
    "        case 'J_total':\n",
    "            ylabel = r'Compliance $J(t)$ [1/MPa]'\n",
    "        case 'J_c':\n",
    "            ylabel = r'Creep compliance $J_c(t)$ [1/MPa]'\n",
    "        case 'J_relative':\n",
    "            ylabel = r'Relative creep compliance $J_c(t) / J_0$ [-]'\n",
    "        case _:\n",
    "            raise ValueError(f\"Invalid compliance type '{settings['compliance_type']}'.\")\n",
    "\n",
    "    # Add labels\n",
    "    axs[0].set_title('Sample strains')\n",
    "    axs[1].set_title('Master creep curve')\n",
    "    axs[2].set_title('Time shifts')\n",
    "    axs[0].set_xlabel('Experimental time [s]')\n",
    "    axs[1].set_xlabel('Master curve time [ln(s)]')\n",
    "    axs[2].set_xlabel('Moisture content [%]')\n",
    "    axs[0].set_ylabel(ylabel)\n",
    "    axs[2].set_ylabel('Time shift ln(a)')\n",
    "\n",
    "    # Add legend\n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    unique = dict(zip(labels, handles)) # remove duplicate legend labels\n",
    "    n = len(unique) - 1\n",
    "    order = [int(i/2) if i%2 == 0 else int((n/2)+i//2+1) for i in range(n) ] + [int(n/2)]\n",
    "    values, keys = list(unique.values()), list(unique.keys())\n",
    "    values, keys = zip(*[(values[i], keys[i]) for i in order])\n",
    "    axs[1].legend(values, keys, loc='upper left', title='Data type (MC)', fontsize='small', title_fontsize='small')\n",
    "\n",
    "    # Display parameters\n",
    "    display(df_master.loc[constant])\n",
    "    display(df_time_shift.loc[constant].rename('time_shift').to_frame().T)\n",
    "    df_export_tmsp_master[0] = exp_data_master\n",
    "    df_export_tmsp_ln_a[0] = df_ln_a.loc[constant]\n",
    "\n",
    "\n",
    "# Export function\n",
    "def export_csv_tmsp(button):\n",
    "    if len(df_export_tmsp_master) > 0 and len(df_export_tmsp_ln_a) > 0:\n",
    "        export_path_master = f'export_creep_master_{w_tmsp_component.value}.csv'\n",
    "        export_path_ln_a = f'export_creep_time_shift_{w_tmsp_component.value}.csv'\n",
    "        df_export_tmsp_master[0].to_csv(export_path_master)\n",
    "        df_export_tmsp_ln_a[0].to_csv(export_path_ln_a)\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        with w_tmsp.children[-1]:\n",
    "            w_tmsp.children[-1].clear_output()\n",
    "            print(f'({timestamp}) Successfully saved data to {export_path_master} and {export_path_ln_a}.')\n",
    "button_tmsp = widgets.Button(description='Export CSVs', button_style='primary', icon='save')\n",
    "button_tmsp.on_click(export_csv_tmsp)\n",
    "display(button_tmsp)\n",
    "\n",
    "\n",
    "# Initialize figure\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 4), gridspec_kw=dict(left=0.12, right=0.98, top=0.94, wspace=0.25, width_ratios=[0.36, 0.36, 0.28]))\n",
    "\n",
    "# Call function\n",
    "constants = [\n",
    "    'E_cRR', 'E_tRR', 'E_avgRR', 'E_cTT', 'E_tTT', 'E_avgTT', 'E_cLL', 'E_tLL', 'E_avgLL',\n",
    "    'G_RT', 'G_TR', 'G_avgRT', 'G_RL', 'G_LR', 'G_avgRL', 'G_TL', 'G_LT', 'G_avgTL',\n",
    "    'E_cRT', 'E_tRT', 'E_avgRT', 'E_cRL', 'E_tRL', 'E_avgRL', 'E_cTL', 'E_tTL', 'E_avgTL'\n",
    "]\n",
    "w_tmsp_component = widgets.Dropdown(options=constants, description='component')\n",
    "w_tmsp = widgets.interactive(\n",
    "    visualize_tmsp_model,\n",
    "    constant=w_tmsp_component,\n",
    "    moisture=widgets.FloatSlider(value=12.5, min=0, max=25, step=0.1),\n",
    "    show_report=False,\n",
    "    show_confidence=False,\n",
    "    df_master=widgets.fixed(df_master),\n",
    "    df_time_shift=widgets.fixed(df_time_shift),\n",
    "    df_ln_a=widgets.fixed(df_ln_a),\n",
    "    df_log=widgets.fixed(df_log),\n",
    "    df_models_master=widgets.fixed(df_models_master),\n",
    "    df_models_time_shift=widgets.fixed(df_models_time_shift),\n",
    "    df_meta=widgets.fixed(df_meta),\n",
    "    df_creep=widgets.fixed(df_creep),\n",
    "    axs=widgets.fixed(axs)\n",
    ")\n",
    "display(w_tmsp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d36c784-283c-4f59-8d4a-2ec9d79c87d1",
   "metadata": {},
   "source": [
    "## Data export\n",
    "---\n",
    "Export the mean coefficients and their standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cb8f59-5b43-40b6-b4d1-541a02e9f157",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define data path\n",
    "export_path = input_folder / 'viscoelasticity_coefficients.csv'\n",
    "\n",
    "# Assemble data\n",
    "df_export = df.drop(columns=['tau']).groupby(['rh', 'type', 'tau'])[['g', 'C_inv', 'mc', 'R2']].agg(['mean', 'std'])\n",
    "df_export = df_export.loc[ [not ('cL_' in type_name or 'tL_' in type_name) for type_name in df_export.index.get_level_values('type')] ]\n",
    "df_export.index = pd.MultiIndex.from_tuples( [(i[0], i[1].replace('x_', '_'),) + i[2:] for i in df_export.index], names=df_export.index.names )\n",
    "\n",
    "taus = df_export.index.get_level_values('tau').unique()\n",
    "series_g, series_g_std, series_tau = [], [], []\n",
    "for i, tau in enumerate(taus):\n",
    "    df_tau = df_export.query(f'tau == {tau}')\n",
    "    data = df_tau.droplevel('tau')\n",
    "    series_g.append( pd.Series(data=data[('g', 'mean')], name=f'g_{i}') )\n",
    "    series_g_std.append( pd.Series(data=data[('g', 'std')], name=f'Std_g_{i}') )\n",
    "    series_tau.append( pd.Series(data=df_tau.index.get_level_values('tau'), index=data.index, name=f'tau_{i}') )\n",
    "\n",
    "series = series_tau + series_g + series_g_std\n",
    "# series.append(pd.Series(data=df_export.groupby(['rh', 'type']).mean()[('mc', 'mean')], name='mc'))\n",
    "series.append(pd.Series(data=df_export.groupby(['rh', 'type']).mean()[('R2', 'mean')], name='R2'))\n",
    "\n",
    "df_export_pretty = pd.concat(series, axis=1)\n",
    "\n",
    "# Save data\n",
    "df_export_pretty.to_csv(export_path)\n",
    "print(f'Successfully saved file {export_path}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d66da65-0dab-4a16-be3a-682658fb1e7e",
   "metadata": {},
   "source": [
    "Export the results of the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b650cf02-7640-417b-898c-34fa47b3c1ad",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define data path\n",
    "clust_export_path = input_folder / 'cluster.csv'\n",
    "\n",
    "# Save data\n",
    "clust_df.to_csv(clust_export_path)\n",
    "print(f'Successfully saved file {clust_export_path}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a950a4fc-a631-4bc1-8b6d-7d79dd47d42a",
   "metadata": {},
   "source": [
    "## References\n",
    "---\n",
    "\n",
    "<ul>\n",
    "    <li><a href='https://doi.org/10.1007/s11043-023-09604-0'>Bengtsson, R., Bergeron, L., Afshar, R., Mousavi, M., Gamstedt, E.K., 2024. Evaluating the viscoelastic shear properties of clear wood via off-axis compression testing and digital-image correlation. Mech Time-Depend Mater 28, 2069–2083.</a></li>\n",
    "    <li><a href='https://doi.org/10.1007/BF02472963'>Hayashi, K., Felix, B., Le Govic, C., 1993. Wood viscoelastic compliance determination with special attention to measurement problems. Mater. Struct. 26, 370–376.</a></li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
